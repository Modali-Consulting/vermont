{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Content saved in website_content directory\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin\n",
    "import re\n",
    "\n",
    "# Function to sanitize filenames\n",
    "def sanitize_filename(filename):\n",
    "    # Remove query parameters from URLs\n",
    "    filename = filename.split('?')[0]\n",
    "    # Replace any other potentially problematic characters\n",
    "    return re.sub(r'[<>:\"/\\\\|?*]', '_', filename)\n",
    "\n",
    "def save_file_from_url(url, directory, filename):\n",
    "    response = requests.get(url)\n",
    "    response.raise_for_status()\n",
    "    # Sanitize the filename to avoid file system errors\n",
    "    safe_filename = sanitize_filename(filename)\n",
    "    with open(os.path.join(directory, safe_filename), 'wb') as file:\n",
    "        file.write(response.content)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Create a directory for the scraped content\n",
    "directory_name = 'website_content'\n",
    "if not os.path.exists(directory_name):\n",
    "    os.makedirs(directory_name)\n",
    "\n",
    "# URL of the webpage to scrape\n",
    "url = 'https://www.nyclu.org/en/nypd-traffic-stops-data'\n",
    "\n",
    "# Send a GET request to the webpage\n",
    "response = requests.get(url)\n",
    "response.raise_for_status()\n",
    "\n",
    "# Parse the HTML content of the page using BeautifulSoup\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "# Save the main HTML content\n",
    "with open(os.path.join(directory_name, 'index.html'), 'w', encoding='utf-8') as file:\n",
    "    file.write(soup.prettify())\n",
    "\n",
    "# Find and save all linked CSS files\n",
    "for css_link in soup.find_all(\"link\", rel=\"stylesheet\"):\n",
    "    css_href = css_link.get('href')\n",
    "    if css_href:\n",
    "        css_url = urljoin(url, css_href)\n",
    "        css_filename = os.path.basename(css_href)\n",
    "        save_file_from_url(css_url, directory_name, css_filename)\n",
    "\n",
    "# Find and save all linked JS files\n",
    "for js_script in soup.find_all(\"script\", src=True):\n",
    "    js_src = js_script.get('src')\n",
    "    if js_src:\n",
    "        js_url = urljoin(url, js_src)\n",
    "        js_filename = os.path.basename(js_src)\n",
    "        save_file_from_url(js_url, directory_name, js_filename)\n",
    "\n",
    "print(f\"Content saved in {directory_name} directory\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#shell command to update all the files into a site\n",
    "$counter = 1\n",
    "Get-ChildItem -Path .\\*.css | ForEach-Object {\n",
    "    $originalName = $_.Name\n",
    "    $newName = \"style$counter.css\"\n",
    "    Rename-Item $originalName -NewName $newName\n",
    "    (Get-Content index.html) -replace $originalName, $newName | Set-Content index.html\n",
    "    $counter++\n",
    "}\n",
    "\n",
    "$counter = 1\n",
    "Get-ChildItem -Path .\\*.js | ForEach-Object {\n",
    "    $originalName = $_.Name\n",
    "    $newName = \"script$counter.js\"\n",
    "    Rename-Item $originalName -NewName $newName\n",
    "    (Get-Content index.html) -replace $originalName, $newName | Set-Content index.html\n",
    "    $counter++\n",
    "}\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
